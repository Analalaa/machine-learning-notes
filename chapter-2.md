## 过拟合
过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施
过拟合也是无法彻底避免的，只能“缓解”、减小风险，


## 评估方法
> 对模型的**泛化误差**进行评估，选择最小的那个模型

测试集(testing set)上跑出测试误差(testing error)作为泛化误差的近似。
测试样本通常也从样本真是分布中独立同分布采样得来，但注意要尽可能与训练集(training set)互斥

### 留出法
training set:testing set = 7:3(大约 2/3 ～ 4/5 用于训练)
* 注意保证样本的类别比例相似，例如正反例的比例保持一致啊等等，有很多种划分方法
* 单次使用留出法的估计结果往往不够可靠，一般要采取若干次随机划分、重复进行试验评估后做均值作为评估结果

### k折交叉验证法
数据集划分为k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性，每次用 k-1 个子集的并集作为训练集，余下的子集作为测试集
同样通常要随机使用不同的划分方法重复p次，以均值作为k折交叉验证的评估结果

### 自助法
数据集 D，包含 m 个样本，对 D 采样 得到 D'：每次随机从 D 中挑选一个样本，将其深拷贝放入 D'，然后再将该样本放回初始数据集 D 中，是的该样本仍有可能被采样到。
* 自助法在数据集较小、难以有效划分 training/testing 时很有用
* 自助法能从初始数据集中产生多个不同的训练集，这对集成学习有很大好处
* 自助法改变了初始数据集的分布，会引入估计偏差，初始数据量足够时，留出法和交叉验证法更加常用

---
验证集(validation set)是用于模型评估与选择中，用于评估测试的数据集
测试集(testing set)是学得的模型在实际使用中遇到的数据成为测试数据

## 性能度量
回归任务最常用的性能度量是“均方误差”，有离散和连续两种表达形式
1. 错误率与精度
2. 查准率(precision)、查全率(recall)与 F1-socre
  混淆矩阵(confusion matrix)
  P-R曲线(查准率-查全率曲线)
  * 两个学习器的P-R曲线交叉，比较P-R曲线下面积的大小。
  * 平衡点(BEP)，越大，学习器越优，平衡点是 P=R 的点
  * F1-score，p-r的加权调和平均
多个二分类混淆矩阵或多次训练/测试得到的混淆矩阵或多个数据集上进行训练/测试，我们希望在n个二分类混淆矩阵上综合考察p和r
    > 1. 先在各cm上分别算p和r，再求均值，得到“宏查准/全率”(macro-P),“宏F1”(macro-F1)
    > 2. 先将各cm的四个对应元素进行平均，再基于这些平均值计算出对应“微查准/全率”(micro-P/R)，“微F1”（micro-F1）
3. ROC与AUC
ROC(Receiver Operation Characteristic)”受试者工作特征曲线“的纵轴：真正例率，TPR；横轴：假正例率，FPR
* P-R图相似，学习器能包住其他学习器的这个，性能较优
* 若ROC交叉，比较ROC曲线下的面积，即 AUC（area under ROC curve）
* AUC考虑的是样本预测的排序质量，与排序误差优紧密联系，loss考虑每一对儿正反例，正例预测值小于反例就记一个“1个惩罚”，如果相等记”0.5个惩罚“，lrank对应的是ROC曲线之上的面积；
4. 代价敏感错误率与代价曲线

## 比较检验
统计假设检验(hypothesis test)，基于假设检验结果可以推断出，若在testing set上观察到学习器A比B好，则A的泛化性能比在统计意义上优于B的把握有多大。
以错误率为性能度量
### 假设检验
"假设"是对学习器泛化错误率分布的某种判断或猜想，可以根据测试错误率估推出泛化错误率达分布
对于推导出的 m 个样本的testing set上，泛化错误率为 u 的学习器被测得测试错误率为 u0 的概率，符合二项分布，有二项检验的结论，a的显著度下，u <= u0 不能被拒绝，即在 a 的显著度下可以认为学习器的泛化错误率大于 u0
同样是多次使用留出法或交叉检验法，运用"t-test"：假定得到了k个测试错误率，则平均测试错误率和方差都可以知道，考虑到k个测试错误率可以看作泛化错误率达独立采样，则变量服从自由度为 k-1 的t分布
* 以上两种方法都是对于单个学习器泛化性能的假设进行检验
### 交叉验证t检验
对于两个学习器 A B，使用 k折交叉验证法得到测试错误率，对每一对儿结果求差，根据差值用"paried t-tests"进行比较检验，计算出差值对均值和方差，
若变量 t 小于临界值，则假设不能被拒绝，即认为两个学习器的性能没有显著差别；否则可以认为两个学习器的性能有显著差别，切平均错误率较小的学习器更优。
* 重要前提：测试错误率均为泛化错误率达独立采样
* 现实情况：使用交叉验证等的时候可能因为样本有限，从而不同轮次的训练集有一定程度的重叠，导致测试错误率实际上不独立，导致过高估计假设成立的概率。
解决办法： "5*2 交叉验证"：做5次2折交叉验证 [Dietterich, 1998]
### MCNemar 检验
对于二分类，留出法可以估计出“分类差别列联表”，如果两个学习器性能相同，那么$|e_{01} + e_{10}|$应该服从"卡方分布"
### Friedman 检验 与 Nemenyi 后续检验

## 偏差与方差
bias-variance decomposition （偏差-方差分解）是解释学习算法泛化性能的一种重要工具
偏差度量了学习算法的**期望预测**与**真实结果**的偏离程度，也就是刻画了学习算法本身的拟合能力
方差度量了同样大小的训练集的变动所导致的学习性能的变化，也就是刻画了数据扰动所造成的影响
噪声表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度
偏差-方差分解说明泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的

---
**2.4 试述真正例率（TPR）、假正例率（FPR）与查准率（P）、查全率（R）之间的联系**
F1 是在特定阈值下计算的，而 BEP 是找出 P=R 时的那个点。
* A 的 F1 分数可能来自一个 P ≠ R 的点。
* B 虽然 F1 低，但可能在 P=R 的点上有更高的值。\

BEP 是一条 PR 曲线上的一个点，而 F1 是某个点的一个数值。
* PR 曲线形状不同，F1 和 BEP 之间可能无直接对应关系。
* 比如某模型 F1 较高，但它的 PR 曲线非常“尖锐”，在 P=R 处的值反而较低。
---
**2.6 错误率与ROC的关系**
1. ROC 曲线本质是 FPR 与 TPR 的变化图；
2. 错误率中包含 FP 和 FN，而 FPR 和 TPR 也分别来自 FP、FN 的分布；
3. 在不同的分类阈值下，ROC 曲线的每个点都对应一组 (FPR, TPR)，同时也对应一个错误率值。
---
**2.7 试证明任意一条ROC曲线都有一条代价曲线与之对应，反之亦然**

**经典结论：ROC 曲线 与 成本/代价曲线（Cost Curve）是可以相互转换的一一对应关系。**
- **ROC 曲线**体现的是模型在不同阈值下的**分类能力**；
- **代价曲线（Cost Curve）**反映的是模型在不同**代价权重或类别比例**下的**期望损失表现**。
---
代价曲线（Cost Curve）

用于衡量模型在不同代价场景下的期望损失：

$$
\text{Expected Cost} = c_1 \cdot \text{FNR} \cdot \pi + c_2 \cdot \text{FPR} \cdot (1 - \pi)
$$

其中：
- $\pi$：正类先验概率；
- $c_1, c_2$：正类 / 负类错分的单位代价；
- $\text{FNR} = 1 - \text{TPR}$

---
**ROC → Cost Curve**

1. ROC 曲线包含所有 $(\text{FPR}(t), \text{TPR}(t))$ 对应点；
2. 给定某一代价比例（或先验 + 代价比），可以从 ROC 曲线上选出最优点（最小期望成本）；
3. 利用该点计算代价，即得代价曲线上的一个点；
4. 随着代价比例从 0 到 1 变化，ROC 曲线生成完整的代价曲线。

**Cost Curve → ROC**

1. 假设我们知道某代价比例下的最小期望成本点；
2. 反推出该点对应的 $\text{FPR}$、$\text{TPR}$，得到 ROC 曲线上的一个点；
3. 对所有代价比例重复上述过程，得出完整 ROC 曲线。

---
**几何直觉**

- ROC 曲线：在 2D 空间中展示分类器区分正负样本的能力；
- Cost 曲线：是在代价空间中寻找每个代价比下的“最优解”；
- ROC 曲线的**切线斜率**与代价比例直接相关；
- Cost 曲线相当于把 ROC 空间中的最优点映射到一维“代价空间”中。


